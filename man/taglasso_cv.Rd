% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/taglasso_cv.R
\name{taglasso_cv}
\alias{taglasso_cv}
\title{K-fold cross-validation for tag-lasso estimation of the precision matrix}
\usage{
taglasso_cv(
  X,
  A,
  pendiag = F,
  seed,
  fold = 5,
  lambda1 = NULL,
  l1length = 10,
  l1gran = 10^2,
  l1max = 5,
  lambda2 = NULL,
  l2length = 10,
  l2gran = 10^2,
  l2max = max(max(stats::cor(X) - diag(ncol(X))), -min(stats::cor(X) - diag(ncol(X)))),
  rho = 10^-2,
  it_in = 100,
  it_out = 10,
  it_in_refit = 100,
  it_out_refit = 10,
  do_parallel = TRUE,
  nc = 1
)
}
\arguments{
\item{X}{An (\eqn{n}x\eqn{p})-matrix of \eqn{p} variables and \eqn{n} observations}

\item{A}{An (\eqn{p}x\eqn{|T|})- binary matrix incorporating the tree-based aggregation structure}

\item{pendiag}{Logical indicator whether or not to penalize the diagonal in Omega. The default is \code{TRUE} (penalization of the diagonal)}

\item{seed}{Set the seed to ensure reproducible results for the K-fold cross-validation exercise}

\item{fold}{Set the fold for the cross-validation exercise. Default is fold = 5 for 5-fold cross-validation}

\item{lambda1}{Numeric vector for the aggregation tuning parameter. Default is \code{NULL}, then the program determines this internally.}

\item{l1length}{Number of aggregation tuning parameters to be considered. Default is 10}

\item{l1gran}{Ratio of largest to smallest aggregation tuning parameter. Default is 10^2}

\item{l1max}{Largest value of the aggregation tuning parameter to be considered. Default: the program determines this internally based on starting estimate of 5.}

\item{lambda2}{Numeric vector for the sparsity tuning parameter. Default is \code{NULL}, then the program determines this internally.}

\item{l2length}{Number of sparsity tuning parameters to be considered. Default is 10}

\item{l2gran}{Ratio of largest to smallest sparsity tuning parameter. Default is 10^2}

\item{l2max}{Largest value of the sparsity tuning parameter to be considered. Default: the program determines this internally.}

\item{rho}{Starting value for the LA-ADMM tuning parameter. Default is 10^2; will be locally adjusted via LA-ADMM}

\item{it_in}{Number of inner stages of the LA-ADMM algorithm. Default is 100}

\item{it_out}{Number of outer stages of the LA-ADMM algorithm. Default is 10}

\item{it_in_refit}{Number of inner stages of the LA-ADMM algorithm for re-fitting. Default is 100}

\item{it_out_refit}{Number of outer stages of the LA-ADMM algorithm for re-fitting. Default is 10}

\item{do_parallel}{Logical indicator whether K-fold cross-validation should be executed in parallel with OpenMP or not. Default is \code{FALSE}}

\item{nc}{Number of cores to be used in the parallel loop.}
}
\value{
A list with the following components
\item{\code{l1vec}}{Numeric vector of aggregation tuning parameters}
\item{\code{l2vec}}{Numeric vector of sparsity tuning parameters}
\item{\code{l1opt}}{Optimal aggregation tuning parameter, as selected by K-fold cross-validation}
\item{\code{l2opt}}{Optimal sparsity tuning parameter, as selected by K-fold cross-validation}
\item{\code{cvobj}}{Cross-validation scores along the two-dimensional grid}
}
\description{
This function performs K-fold cross-validation to select the tuning parameters lambda1 and lambda2
}
